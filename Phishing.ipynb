{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import matplotlib.patches as mpatches\n",
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import collections\n",
    "from imblearn.datasets import fetch_datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from scipy.io import arff\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/Bjoern/Desktop/Summer/Work on Datasets/Training Dataset.arff'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c931f828af17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadarff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/Bjoern/Desktop/Summer/Work on Datasets/Training Dataset.arff'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/scipy/io/arff/arffread.py\u001b[0m in \u001b[0;36mloadarff\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0mofile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         \u001b[0mofile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_loadarff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mofile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/Bjoern/Desktop/Summer/Work on Datasets/Training Dataset.arff'"
     ]
    }
   ],
   "source": [
    "data = arff.loadarff('/Users/Bjoern/Desktop/Summer/Work on Datasets/Training Dataset.arff')\n",
    "df = pd.DataFrame(data[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inting dataset\n",
    "for i in df:\n",
    "    df.loc[:,i] = df.loc[:,i].astype(int)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# mean, sdv, etc.\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for null vals\n",
    "df.isnull().sum().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list features\n",
    "df.columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of Phising:Not Phishing\n",
    "print(\"Phishing: \" + str(round(len(df[df['Result'] == 1])/len(df) * 100,2)) + \"%\")\n",
    "print(\"Not Phishing: \" + str(round(len(df[df['Result'] == -1])/len(df) * 100,2)) + \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphed version of distribution\n",
    "colors = [\"#0101DF\", \"#DF0101\"]\n",
    "\n",
    "sns.countplot('Result', data=df, palette=colors)\n",
    "plt.title('Result Distributions \\n (-1: No Phising || 1: Phising)', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Result',axis = 1)\n",
    "y = df['Result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce a histgram of the data vs how many there are\n",
    "\n",
    "# There probably is a better way to do this but I haven't figured it out\n",
    "\n",
    "fig, ax = plt.subplots(6,5,figsize = (30,30))\n",
    "ax = ax.flatten()\n",
    "\n",
    "having_IP_Address = df['having_IP_Address'].values\n",
    "URL_Length = df['URL_Length'].values\n",
    "Shortining_Service = df['Shortining_Service'].values\n",
    "having_At_Symbol = df['having_At_Symbol'].values\n",
    "double_slash_redirecting = df['double_slash_redirecting'].values\n",
    "Prefix_Suffix = df['Prefix_Suffix'].values\n",
    "having_Sub_Domain = df['having_Sub_Domain'].values\n",
    "SSLfinal_State = df['SSLfinal_State'].values\n",
    "Domain_registeration_length = df['Domain_registeration_length'].values\n",
    "Favicon = df['Favicon'].values\n",
    "port = df['port'].values\n",
    "HTTPS_token = df['HTTPS_token'].values\n",
    "Request_URL = df['Request_URL'].values\n",
    "URL_of_Anchor = df['URL_of_Anchor'].values\n",
    "Links_in_tags = df['Links_in_tags'].values\n",
    "SFH = df['SFH'].values\n",
    "Submitting_to_email = df['Submitting_to_email'].values\n",
    "Abnormal_URL = df['Abnormal_URL'].values\n",
    "Redirect = df['Redirect'].values\n",
    "on_mouseover = df['on_mouseover'].values\n",
    "RightClick = df['RightClick'].values\n",
    "popUpWidnow = df['popUpWidnow'].values\n",
    "Iframe = df['Iframe'].values\n",
    "age_of_domain = df['age_of_domain'].values\n",
    "DNSRecord = df['DNSRecord'].values\n",
    "web_traffic = df['web_traffic'].values\n",
    "Page_Rank = df['Page_Rank'].values\n",
    "Google_Index = df['Google_Index'].values\n",
    "Links_pointing_to_page = df['Links_pointing_to_page'].values\n",
    "Statistical_report = df['Statistical_report'].values\n",
    "\n",
    "array_names = ['having_IP_Address', 'URL_Length', 'Shortining_Service',\n",
    "       'having_At_Symbol', 'double_slash_redirecting', 'Prefix_Suffix',\n",
    "       'having_Sub_Domain', 'SSLfinal_State', 'Domain_registeration_length',\n",
    "       'Favicon', 'port', 'HTTPS_token', 'Request_URL', 'URL_of_Anchor',\n",
    "       'Links_in_tags', 'SFH', 'Submitting_to_email', 'Abnormal_URL',\n",
    "       'Redirect', 'on_mouseover', 'RightClick', 'popUpWidnow', 'Iframe',\n",
    "       'age_of_domain', 'DNSRecord', 'web_traffic', 'Page_Rank',\n",
    "       'Google_Index', 'Links_pointing_to_page', 'Statistical_report']\n",
    "\n",
    "\n",
    "array_vars = [having_IP_Address, URL_Length, Shortining_Service,\n",
    "       having_At_Symbol, double_slash_redirecting, Prefix_Suffix,\n",
    "       having_Sub_Domain, SSLfinal_State, Domain_registeration_length,\n",
    "       Favicon, port, HTTPS_token, Request_URL, URL_of_Anchor,\n",
    "       Links_in_tags, SFH, Submitting_to_email, Abnormal_URL,\n",
    "       Redirect, on_mouseover, RightClick, popUpWidnow, Iframe,\n",
    "       age_of_domain, DNSRecord, web_traffic, Page_Rank,\n",
    "    Google_Index, Links_pointing_to_page, Statistical_report]\n",
    "\n",
    "for i in range(len(array_names)):\n",
    "    sns.distplot(array_vars[i], ax = ax[i])\n",
    "    ax[i].set_title(array_names[i], fontsize=14)\n",
    "    ax[i].set_xlim([min(array_vars[i]), max(array_vars[i])])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the correlation matrix between each feature\n",
    "fig, ax = plt.subplots(figsize=(14,8))\n",
    "\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr, cmap='coolwarm_r',annot_kws={'size':20})\n",
    "ax.set_title('Correlation Matrix', fontsize =14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# PCA/TSNE, 2D visualisation\n",
    "\n",
    "# TSNE (also takes long time > 5 mins), maybe try MultiTSNE but need to install a bunch of different packages\n",
    "X_reduced_tsne = TSNE(n_components=2, random_state=42).fit_transform(X.values)\n",
    "\n",
    "# PCA Implementation\n",
    "X_reduced_pca = PCA(n_components=2, random_state=42).fit_transform(X.values)\n",
    "\n",
    "# TruncatedSVD\n",
    "X_reduced_svd = TruncatedSVD(n_components=2, algorithm='randomized', random_state=42).fit_transform(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot reduced data\n",
    "\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24,6))\n",
    "# labels = ['No Fraud', 'Fraud']\n",
    "f.suptitle('Clusters using Dimensionality Reduction', fontsize=14)\n",
    "\n",
    "\n",
    "blue_patch = mpatches.Patch(color='#0A0AFF', label='Not Phishing')\n",
    "red_patch = mpatches.Patch(color='#AF0000', label='Phishing')\n",
    "\n",
    "\n",
    "# t-SNE scatter plot\n",
    "ax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == -1), cmap='coolwarm', label='No Phishing', linewidths=2)\n",
    "ax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 1), cmap='coolwarm', label='Phishing', linewidths=2)\n",
    "ax1.set_title('t-SNE', fontsize=14)\n",
    "\n",
    "ax1.grid(True)\n",
    "\n",
    "ax1.legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "# PCA scatter plot\n",
    "ax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == -1), cmap='coolwarm', label='No Phishing', linewidths=2)\n",
    "ax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == 1), cmap='coolwarm', label='Phishing', linewidths=2)\n",
    "ax2.set_title('PCA', fontsize=14)\n",
    "\n",
    "ax2.grid(True)\n",
    "\n",
    "ax2.legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "# TruncatedSVD scatter plot\n",
    "ax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(y == -1), cmap='coolwarm', label='No Phishing', linewidths=2)\n",
    "ax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(y == 1), cmap='coolwarm', label='Phishing', linewidths=2)\n",
    "ax3.set_title('Truncated SVD', fontsize=14)\n",
    "\n",
    "ax3.grid(True)\n",
    "\n",
    "ax3.legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA, not recommended but this is just to see how dimensionality reduction would work\n",
    "components = 29\n",
    "while True: # make sure it runs at least once\n",
    "    X_pca = PCA(n_components = components).fit(X.values) \n",
    "    if (X_pca.noise_variance_ > 0.1):\n",
    "        break\n",
    "    components -= 1\n",
    "components += 1  # we end with variance less than 90%, so add 1 to n_components to get > 90%\n",
    "X_pca = PCA(n_components = components).fit(X.values)  # X_pca is the reduced feature \n",
    "print(X_pca.noise_variance_, components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Recursive feature elimination\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "rfe = RFE(model, n_features_to_select = 16)\n",
    "rfe = rfe.fit(X_train,y_train)\n",
    "\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)\n",
    "print(rfe.score(X_train,y_train))\n",
    "\n",
    "X_reduced = rfe.transform(X_train)\n",
    "y_pred = rfe.predict(X_train)\n",
    "print(f1_score(y_train,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram of importance\n",
    "\n",
    "plt.figure(figsize=(10,8),dpi = 80)\n",
    "plt.bar(range(1,31),rfe.ranking_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whats the best\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "training_score = cross_val_score(model, X_train, y_train, cv=5)\n",
    "print(round(training_score.mean(),5))\n",
    "\n",
    "print(round(cross_val_score(model, X_reduced, y_train, cv=5).mean(),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
